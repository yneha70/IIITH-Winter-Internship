{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUJp4xSDzZulHxPxKxw0Ii",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yneha70/IIITH/blob/main/Task_2_Performance_Evalution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYKQdDFbzizd"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import m\n",
        "image_folder = r\"C:\\Users\\Neha\\Desktop\\YOLO_Restaurant_Project\\images\"\n",
        "output_folder = r\"C:\\Users\\Neha\\Desktop\\YOLO_Restaurant_Project\\outputs\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "metrics_folder = os.path.join(output_folder, \"performance_metrics\")\n",
        "os.makedirs(metrics_folder, exist_ok=True)\n",
        "\n",
        "model = YOLO(\"yolov8l.pt\")\n",
        "\n",
        "target_objects = {\n",
        "    \"Restaurant.jpg\": \"person\",\n",
        "    \"Train.jpg\": \"train\",\n",
        "    \"Dog.jpg\": \"dog\",\n",
        "    \"Cycling.jpg\": \"bicycle\"\n",
        "}\n",
        "\n",
        "image_names = []\n",
        "f1_scores = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "\n",
        "class_labels = [\"person\", \"train\", \"dog\", \"bicycle\"]\n",
        "cm = np.zeros((4, 4), dtype=int)\n",
        "\n",
        "BOUNDARY_THICKNESS = 2\n",
        "FONT_SCALE = 0.45\n",
        "FONT_THICKNESS = 1\n",
        "LABEL_BG_ALPHA = 0.7\n",
        "\n",
        "for image_name in os.listdir(image_folder):\n",
        "    if not image_name.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
        "        continue\n",
        "\n",
        "    image_path = os.path.join(image_folder, image_name)\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"⚠️ Skipping unreadable image: {image_name}\")\n",
        "        continue\n",
        "\n",
        "    results = model.predict(source=image_path, conf=0.25, imgsz=1280, device=\"cpu\", save=False)\n",
        "    boxes = results[0].boxes\n",
        "    detected_classes = [model.names[int(c)].lower() for c in boxes.cls]\n",
        "\n",
        "    true_label = target_objects.get(image_name, None)\n",
        "    if not true_label:\n",
        "        continue\n",
        "\n",
        "    for det_class in detected_classes:\n",
        "        if det_class in class_labels:\n",
        "            true_idx = class_labels.index(true_label)\n",
        "            pred_idx = class_labels.index(det_class)\n",
        "            cm[true_idx, pred_idx] += 1\n",
        "\n",
        "    TP = detected_classes.count(true_label)\n",
        "    FP = len(detected_classes) - TP\n",
        "    FN = 1 if TP == 0 else 0\n",
        "    precision = TP / (TP + FP + 1e-6)\n",
        "    recall = TP / (TP + FN + 1e-6)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n",
        "\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "    image_names.append(image_name.split(\".\")[0])\n",
        "\n",
        "    for box, cls in zip(boxes.xyxy, boxes.cls):\n",
        "        class_name = model.names[int(cls)].lower()\n",
        "        if class_name not in [\"person\", \"train\", \"dog\", \"bicycle\"]:\n",
        "            continue\n",
        "        conf = float(boxes.conf[0])\n",
        "        xyxy = [int(x) for x in box.tolist()]\n",
        "        color = (0, 255, 0)\n",
        "        label = f\"{class_name.capitalize()} {conf:.2f}\"\n",
        "        cv2.rectangle(img, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), color, BOUNDARY_THICKNESS)\n",
        "        cv2.putText(img, label, (xyxy[0], xyxy[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, FONT_SCALE, (0, 0, 0), FONT_THICKNESS)\n",
        "\n",
        "    cv2.imwrite(os.path.join(output_folder, f\"detected_{image_name}\"), img)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(image_names, f1_scores, marker='o', label=\"F1 Score\", color='blue')\n",
        "plt.plot(image_names, precisions, marker='o', label=\"Precision\", color='green')\n",
        "plt.plot(image_names, recalls, marker='o', label=\"Recall\", color='red')\n",
        "plt.xlabel(\"Images\")\n",
        "plt.ylabel(\"Metric Value\")\n",
        "plt.title(\"Performance Metrics Comparison\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(metrics_folder, \"performance_metrics_combined.png\"))\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(cm, cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xticks(np.arange(len(class_labels)), class_labels)\n",
        "plt.yticks(np.arange(len(class_labels)), class_labels)\n",
        "\n",
        "for i in range(len(class_labels)):\n",
        "    for j in range(len(class_labels)):\n",
        "        plt.text(j, i, str(cm[i, j]), ha='center', va='center', color='black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(metrics_folder, \"confusion_matrix.png\"))\n",
        "plt.close()\n",
        "\n",
        "print(\"\\n Detection complete. All outputs and graphs saved in:\")\n",
        "print(metrics_folder)\n"
      ]
    }
  ]
}